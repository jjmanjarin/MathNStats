{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAuMiPYL5unN"
   },
   "source": [
    "# ANOVA\n",
    "\n",
    "In this document we are going to develop and implement the basic ideas of the One-way ANOVA for the comparison of different population means. In this context we must say that ANOVA is just a hypothesis testing procedure that considers all the populations at once instead of go using all the possible two populations tests.\n",
    "\n",
    "ANOVA is the usual name that a general class of linear models receive: those in which we only have **categorical explanatory variables**. Also, as a sort of field on its own, it has its own nomeclature:\n",
    "\n",
    " * We will denote as **Factors** to all the predictors (explanatory variables) of the model\n",
    " * We will denote as **Levels** to the observations of the factors, i.e. to all the possible categories\n",
    " * We will denote as **Effects** to the estimated parameters of the regression\n",
    "\n",
    "With respect to this last idea, let's make another observation: with respect to the nature of these effects we may have three different types of models:\n",
    "\n",
    " * **Fixed Effects Models**: in this case the effects (estimated parameters) are constant in time, i.e. they are real numbers\n",
    " * **Random Effects Models**: in this case the estimated effects are random variables\n",
    " * **Mixed Effects Models**: in this case the estimated effects are both, constant and/or random variables\n",
    "\n",
    "All the models we will consider in this document are Fixed Effects, i.e. all the estimated quantities will be constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzYyqx8ft0XU"
   },
   "source": [
    "## General Description\n",
    "\n",
    "Suppose a model in which we want to study if the the belonging to a certain group may affect the value of a response variable. We consider these groups as the different levels of one single categorical variable (with any number of possible levels)\n",
    "\n",
    "We, then, propose the following model\n",
    "\n",
    "\\begin{equation}\n",
    "x_{ij} = m + \\tau_i + e_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "  * $x_{ij}$ is the *j*th observation of the *i*th population\n",
    "  * $\\tau_i$ is the effect of belonging to the *i*th population\n",
    "  * $e_{ij}$ is the random error of each observation\n",
    "  * $m$ is the overall mean of all the observations\n",
    "\n",
    "Remember that in any linear regression model, the response is always the conditional expected value of the response given that the regressors can take a given value.\n",
    "\n",
    "In this context, the main idea is that if there is no difference between the different populations, then the variable $\\tau_i$ must be irrelevant and then the expected value of each observation is the overall mean, up to some random errors. \n",
    "\n",
    "Then, the decision scheme that we use is\n",
    "\n",
    "\\begin{equation}\n",
    "H_0:\\,\\{\\mu_1=\\dots=\\mu_n\\},\\quad H_1:\\,\\{\\text{at least one mean is different}\\}\n",
    "\\end{equation}\n",
    "\n",
    "which is basically an **overall significance test** for the regressors.\n",
    "\n",
    "A technical note is that usually this is a **not of full rank model**, which means that, in general, there is not a unique solution for the estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BQ4z6OFw0m8"
   },
   "source": [
    "## Model Assumptions\n",
    "\n",
    "Just like for any linear model we are going to impose the following conditions:\n",
    "\n",
    " * The residuals must be normally distributed with zero expected value and same variance: $N(0,\\,\\sigma^2)$ (**homoskedasticity** and **zero conditional mean**).\n",
    " * The observations must be **independent**, satisfied as usual if the sample size is at most the 10% of the population size (in any case there is a formal test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWdfgKZuvhJi"
   },
   "source": [
    "## ANalysis Of VAriance\n",
    "\n",
    "Using the first condition, we can take the expected value in the equation of the model and see that there are some valid estimators for the **overall mean**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat m = {\\bar{\\bar x}}\n",
    "\\end{equation}\n",
    "\n",
    "and for the **group effect**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\tau = \\bar x_i -\\bar{\\bar x}\n",
    "\\end{equation}\n",
    "\n",
    "if we plug this last equation in the model above we find an equation for the **residuals**\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat e_{ij} = x_{ij} - \\bar x_i\n",
    "\\end{equation}\n",
    "\n",
    "from here we can find two different sum of squares for this model:\n",
    "\n",
    "  * The **sum of squared residuals** (SSR or SSW)\n",
    "  <br>\n",
    "  \\begin{equation}SSW = \\sum_{ij} \\hat e_{ij}^2 = \\sum_{i,j} (x_{ij} - \\bar x_i)^2\n",
    "  \\end{equation}\n",
    "  \n",
    "   which is known in this context as the Sum of Squares Within groups or **Unexplained Variability**. This last name implies that it is the part of variability that we will always have since it is due to the random nature of our variables, in the end it is nothing more than the variance. Now, what we usually want is this quantity divided by the number of degrees of freedom, a sort of average, in this case, since there are $N$ observations and $K$ populations, we have $N-K$ degrees of freedom, then we define\n",
    "      \n",
    "  \\begin{equation}MSR = \\frac{SSR}{n - K}\\end{equation}\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "  * The **sum of squares between groups** (SSG)\n",
    "  \n",
    "  \\begin{equation}SSG = \\sum_{i} \\hat \\tau_{i}^2 = \\sum_{i} (\\bar x_i -\\bar{\\bar x})^2\\end{equation}\n",
    "  \n",
    "   which is known in this context as the **Explained Variability**, which implies that this is the variability of the response that we will be able to explain using our model. See that it is just the difference from each population mean to the overall mean, then if the model is relevant these differences will be significantly important. Just as before, we can find the degrees of freedom, which in this case it is simply $K-1$, then\n",
    "      \n",
    "    \\begin{equation}MSG = \\frac{SSG}{K-1}\\end{equation}\n",
    "\n",
    "Now we can define an F-ratio as\n",
    "\n",
    "\\begin{equation}F = \\frac{MSG}{MSW}\\end{equation}\n",
    "\n",
    "We use this value as the test statistic, then:\n",
    "\n",
    "  * If the model is useful, and let us explain the variability of the response in terms of the effect of the population, i.e. if there is a significant difference in the mean of the populations, then the $MSG$ will be significantly greater than $MSW$ and then the F-value will be large and its p-value small\n",
    "  \n",
    "  * If the model is not useful, then the $MSW$ will be greater than $MSG$, the F-value will be small and the p-value big. This implies that there is no significant difference bewteen the population means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZXcV_9L8Ttb"
   },
   "source": [
    "## ANOVA in Python\n",
    "\n",
    "Let's perform this analysis of variance in the same dataset we used for the linear models: `forestarea`. Then let's first load the packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2557,
     "status": "ok",
     "timestamp": 1606651453604,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgROm1G9L6BfG7PCIlE0tJxJJ2QITgE4QN52iI2=s64",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "gYaw2qQs9fH3",
    "outputId": "c9a23208-c822-496a-c379-7ba02b38d3ce"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import mstats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqISFXBP93jG"
   },
   "source": [
    "now we load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "executionInfo": {
     "elapsed": 20531,
     "status": "ok",
     "timestamp": 1572941774168,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "3OYF-Wcm944b",
    "outputId": "dbbf9801-acbb-4d31-ff7d-18ce0dc6f34f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9910662a8b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mydrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('mydrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1222,
     "status": "ok",
     "timestamp": 1572941777322,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "bZLwzoTa-KAb",
    "outputId": "79627156-0be1-42d7-d23b-24c7eab8468b"
   },
   "outputs": [],
   "source": [
    "mydata = pd.read_csv(\"/content/mydrive/My Drive/IE Bootcamp - Math & Stats /data/forestarea.csv\")\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iTyNLWB-OUH"
   },
   "source": [
    "Since we saw that the best model we could obtain was with the log transformed response variable (taking `anwwith2014` as response) let's find that variable and add it to the dataset (as well as the other transformed variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1572941780282,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "IKQTk-tk-gpO",
    "outputId": "2c34921f-99c0-4fb5-afef-b4018b5faf14"
   },
   "outputs": [],
   "source": [
    "mydata['lforar2014'] = np.log(mydata.forar2014)\n",
    "mydata['lanwwith2014'] = np.log(mydata.anwwith2014)\n",
    "mydata['lavprec2014'] = np.log(mydata.avprec2014)\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMlc4w-AB8e3"
   },
   "source": [
    "### Replacing Codes\n",
    "\n",
    "Let's replace the `continent` code by their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 749,
     "status": "ok",
     "timestamp": 1572941794230,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "na7g5-w-CAdN",
    "outputId": "3a3215d4-451c-429a-92ad-4e3c43f6a860"
   },
   "outputs": [],
   "source": [
    "mydata['continent'].replace({1: 'Africa', 2: 'America', 3: 'Asia', 4:'Australia', 5:'Europe'}, inplace= True)\n",
    "mydata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biSg4zDjCGpq"
   },
   "source": [
    "### Train/Test splitting\n",
    "\n",
    "Now let's preform the usual 80/20 splitting for the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNX672ElA9mW"
   },
   "outputs": [],
   "source": [
    "rand_state = np.random.RandomState(1)\n",
    "df_train, df_test = train_test_split(mydata, \n",
    "                                   test_size = 0.20,\n",
    "                                   random_state = rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Kw1JZxCWPn"
   },
   "source": [
    "### Descriptive\n",
    "\n",
    "Let's use the `describe` function to have a first look at the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1572941801056,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "_8fkZtfACcmw",
    "outputId": "376f56b1-8743-4868-fe02-37289120d555"
   },
   "outputs": [],
   "source": [
    "df_train.groupby('continent')['lanwwith2014'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCeyC-tsCjD2"
   },
   "source": [
    "Taking a look at the standard deviations, it seems that there are two continents that may be significantly different with respect to the mean and that may also give problems with the equality of variances: Africa and Australia.\n",
    "\n",
    "Note also that from this table:\n",
    "\n",
    "  * the data is unbalanced, i.e. the number of observations is different in each group\n",
    "  * We do not have enough observations in Asia nor in Australia to proceed with them (we should drop them in the analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 902,
     "status": "ok",
     "timestamp": 1572941804395,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "9ZrB3EtJD77O",
    "outputId": "4e9008fd-8f50-4fa1-d424-0ca7f19c743d"
   },
   "outputs": [],
   "source": [
    "dataset = df_train[(df_train['continent'] != 'Asia') & (df_train['continent'] != 'Australia')]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPYfo4seHvBM"
   },
   "source": [
    "From now we will work with these three continents only: Europe, Africa and America"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfmonRG1pq_N"
   },
   "source": [
    "\\begin{equation}\n",
    "E[\\text{response}|\\text{categorical}] = \\beta_0 + \\beta_1\\,\\text{categorical} + \\text{error}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SY9_CvD09ucq"
   },
   "source": [
    "### Checking the Assumptions\n",
    "\n",
    "Since this ANOVA test is made on top of a linear model, we must check the assumptions of these type of models. In this case we will only consider:\n",
    "\n",
    "  * Independency of the observations\n",
    "  * Homoskedasticity of the residuals\n",
    "  * Normality of the residuals\n",
    "\n",
    "All these require that we have a model fitted and then from its residuals check the assumptions. However sometimes it is assumed (although formally wrong) that they may be directly checked with the data. In this case we can use the common rules:\n",
    "\n",
    " * Independency can be assumed if we have a sample with a size less than the 10% of the population size.\n",
    " * Homoskedasticity can be seen graphically with a comparison of boxplots and analytically with a Levene or Bartlett test (comparisons of multiple variances)\n",
    " * Normality can be checked with the normality of the sample (see in section **Failure of Normality** below)\n",
    "\n",
    "Let's see how this may work\n",
    "\n",
    " * The sample is of size 34 (in the `df_train` once the NaN have been dropped) while the total number of countries is 195. This is the 17% so in this case we may have problems with correlations between different data\n",
    " * For the homoskedasticity we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1572941809590,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "Iq5oimwQ9zKw",
    "outputId": "6c78e281-2298-4130-aa33-f9cf6f73231b"
   },
   "outputs": [],
   "source": [
    "ss.levene(dataset['lanwwith2014'][dataset['continent'] == 'Africa'].dropna(),\n",
    "          dataset['lanwwith2014'][dataset['continent'] == 'America'].dropna(),\n",
    "          dataset['lanwwith2014'][dataset['continent'] == 'Europe'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWxSWnp7DMFV"
   },
   "source": [
    "To use Bartlett test we should use\n",
    "\n",
    "```python\n",
    "ss.bartlett(dataset['lanwwith2014'], dataset['continent'])\n",
    "```\n",
    "\n",
    "but with the continent encoded as a numerical variable, i.e. the original encoding of the dataset. We leave that for you.\n",
    "\n",
    "In any case, we can see that both tests return a p-value higher the the common significance levels, then we fail to reject the Null hypothesis and must conclude that we do not find evidence against the equality of variances. See that the boxplots may have led us to a different conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1572941886022,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "Ti-qzFTEDgzF",
    "outputId": "38b7f2e7-873c-40de-fb58-b8e3d26df091"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "g = sns.boxplot(data = dataset, x = 'continent', y = 'lanwwith2014')\n",
    "g.axes.set_title('Annual Water Withdrawal vs. Continent', fontsize = 20)\n",
    "g.axes.set_xlabel('Continent', fontsize = 15)\n",
    "g.axes.set_ylabel('Log(Annual Water Withdrawal)', fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tCnruSt5tTn"
   },
   "source": [
    "## ANOVA - statsmodels\n",
    "\n",
    "Now we define the ols model for the ANOVA test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "executionInfo": {
     "elapsed": 680,
     "status": "ok",
     "timestamp": 1572941890646,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "kdmVcs5U6H95",
    "outputId": "cc842b9b-1bfb-469e-ba0e-6dd33c64ec16"
   },
   "outputs": [],
   "source": [
    "model = ols('lanwwith2014 ~ C(continent)', data = dataset).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkwlOcnB6I-M"
   },
   "source": [
    "From the summary table we see that the base group is **Africa** and all the other continents are significant and relevant to explain the variability of the response. Now the ANOVA table can be found as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1572941896448,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "8IOZChRh6Nyt",
    "outputId": "46206ee4-40f4-4786-9872-c408e2fa130c"
   },
   "outputs": [],
   "source": [
    "aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "aov_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhKdGHtF6RHh"
   },
   "source": [
    "from where we see that the p-value is $0.0145$. So if our test were for a 5%, we Reject the Null hypothesis and conclude that we find evidence against the equality of means from our sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ll8HKMtKytf"
   },
   "source": [
    "## ANOVA - scipy\n",
    "\n",
    "We can also use the `scipy.stats` package where its `f_oneway` function gives the same answer, then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 796,
     "status": "ok",
     "timestamp": 1572941902594,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "GJwhk3QtK15l",
    "outputId": "437da539-7dc9-4379-ce3d-d2b08fa49c72"
   },
   "outputs": [],
   "source": [
    "ss.f_oneway(dataset['lanwwith2014'][dataset['continent'] == 'Africa'].dropna(),\n",
    "            dataset['lanwwith2014'][dataset['continent'] == 'America'].dropna(),\n",
    "            dataset['lanwwith2014'][dataset['continent'] == 'Europe'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-foBHaJLJQb"
   },
   "source": [
    "So the p-value is the same as before and, therefore, the conclusion too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdAJ7keWJOka"
   },
   "source": [
    "## Post-hoc Analysis\n",
    "\n",
    "If we Reject the NULL hypothesis there are two different approaches (there are more) we can take in order to find out which group is actually significantly different:\n",
    "\n",
    "  * Bonferroni: we set a penalty in the significance level and then compute all the two population differences for this new significance level\n",
    "  * Tukey: looks for a **minimum significant difference** (HSD or MSD) that may let us declare two populations as different. To do it we introduce the **Studentised Interquartile Range** distribution or Tukey's-q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1572941937290,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "2r3USXjXhgvA",
    "outputId": "416e6e56-89e8-4fa1-d080-8f385efe5a57"
   },
   "outputs": [],
   "source": [
    "dataset_nona = dataset.dropna()\n",
    "mc = MultiComparison(dataset_nona['lanwwith2014'], dataset_nona['continent'])\n",
    "print(mc.tukeyhsd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njoEBnA8hn2Y"
   },
   "source": [
    "Since the null hypothesis is the equality of means, from the output we see that **Africa** is significantly different to **America** but not to **Europe** and that **America** and **Europe** are not significantly different (all for a 5%)\n",
    "\n",
    "## Failure of Normality\n",
    "\n",
    "To test for normality we can look at the table p-values for JB and Omnibus (0.568 and 0.357) respectively and use the functions we used in linear models to find the p-values for Shapiro-Wilk and D'Agostino tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdgpSrE3M5Ar"
   },
   "outputs": [],
   "source": [
    "def NormalityTests(x, sig_level):\n",
    "    '''\n",
    "    This function computes the p-value and statistics of the Shapiro-Wilk and D'Agostino tests for normality\n",
    "    It also includes the set of libraries to be loaded in the test (no cheks done)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "     - x: array of values of the variable to be tested\n",
    "     - sig_level: significance level to be used in the decision of the test\n",
    "    \n",
    "    Output\n",
    "    \n",
    "     - p-value, statistic and decision for both tests    \n",
    "    '''\n",
    "    from scipy.stats import shapiro\n",
    "    from scipy.stats import normaltest\n",
    "    \n",
    "    shap_stat, shap_p = shapiro(x)\n",
    "    k2_stat, k2_p = normaltest(x)\n",
    "    \n",
    "    print(\"From the Shapiro Wilk test:\\n\\nStatistic: \", shap_stat, \"\\np-value: \", shap_p)\n",
    "    if shap_p > sig_level:\n",
    "        print(\"Fail to reject Normality: No evidence found against normality\\n\\n\")\n",
    "    else:\n",
    "        print(\"Reject Normality: Evidence found against normality\\n\\n\")\n",
    "    \n",
    "    print(\"From the D'Agostino test:\\n\\nStatistic: \", k2_stat, \"\\np-value: \", k2_p)\n",
    "    if k2_p > sig_level:\n",
    "        print(\"Fail to reject Normality: No evidence found against normality\\n\\n\")\n",
    "    else:\n",
    "        print(\"Reject Normality: Evidence found against normality\\n\\n\")\n",
    "             \n",
    "\n",
    "def HisQQplots(x):\n",
    "    '''\n",
    "    This function plots the histogram and qq-plot of an array in order to perform a visual analysis of normality\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "     - x: array to plot\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "     A plot consisting in two subplots (one for each of the previous ones)\n",
    "    '''\n",
    "    # define the different regions\n",
    "    f, (ax_box, ax_hist) = plt.subplots(2, \n",
    "                                        sharex = False, \n",
    "                                        gridspec_kw={\"height_ratios\": (.25, .75)})\n",
    "    f.set_figheight(8)\n",
    "    f.set_figwidth(8)\n",
    "    plt.suptitle('Normality Plots', fontsize = 20)\n",
    "    # Add a graph in each part\n",
    "    sns.distplot(x, hist = True, \n",
    "                 kde = False, \n",
    "                 bins = 10, \n",
    "                 hist_kws={'edgecolor':'black'},\n",
    "                 ax=ax_box)\n",
    "    ss.probplot(x, plot=sns.mpl.pyplot)\n",
    "    plt.tight_layout(rect=(0,0,1,0.94))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNla2qncNmEe"
   },
   "source": [
    "Then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1572941985532,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "o7vlzNCDM7TV",
    "outputId": "4b40bb9f-c036-4233-91eb-944d40b98437"
   },
   "outputs": [],
   "source": [
    "NormalityTests(model.resid, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEejY_tHNneJ"
   },
   "source": [
    "And Graphically we can do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1572941989028,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "93nb_ZNONJaP",
    "outputId": "13c4d4d2-c8f6-4d3d-9135-d44115df39b6"
   },
   "outputs": [],
   "source": [
    "HisQQplots(model.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ajeABRhNj-6"
   },
   "source": [
    "So in our case we do not find any problems with normality of our residuals but sometimes we will. In these cases, we may still perform a test for both:\n",
    "\n",
    " * Equatility of variances: Since Levene test is robust against outliers, can still be used when normality fails\n",
    " * Comparison of population: We use the **Kruskal-Wallis procedure**, a non-parametric test that does not require any normality in the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKPIEy9qVtHq"
   },
   "source": [
    "### Kruskal-Wallis\n",
    "\n",
    "This procedure tests if the **median** of all the populations are equal and the only requirement is that the distributions of the populations are of the same *type*, i.e. all rigth-skewed, all leptokurtic,... In Python there are different implementations of it, we will use the one in **scipy.stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "executionInfo": {
     "elapsed": 730,
     "status": "ok",
     "timestamp": 1572941994953,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "Hu8B_RhmhrKp",
    "outputId": "bd64691c-cb91-446c-c208-5fa19e30c0b7"
   },
   "outputs": [],
   "source": [
    "print(\"Kruskal Wallis H-test test:\\n\")\n",
    "\n",
    "H, pval = mstats.kruskalwallis(dataset['lanwwith2014'][dataset['continent'] == 'Africa'].dropna().values,\n",
    "                               dataset['lanwwith2014'][dataset['continent'] == 'America'].dropna().values,\n",
    "                               dataset['lanwwith2014'][dataset['continent'] == 'Europe'].dropna().values)\n",
    "\n",
    "print(\"H-statistic:\", H)\n",
    "print(\"P-Value:\", pval)\n",
    "\n",
    "if pval < 0.05:\n",
    "    print(\"\\nReject the NULL hypothesis for a 5%: There is evidence in favor of significant differences between the populations.\")\n",
    "if pval > 0.05:\n",
    "    print(\"\\nFail to Reject the NULL hypothesis for a 5%: There is no evidence in favor of significant differences between the populations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW4K0rRqhzpd"
   },
   "source": [
    "The only point we should be careful with when we run this function is that it only accepts arrays as inputs, that's why we have added the **.values** at the end.\n",
    "\n",
    "The p-value is in no contradiction with the previous ANOVA result then we could perform a Post-hoc analysis. In this case we should use any of the following\n",
    "\n",
    " * Conover test.\n",
    " * Dunn test.\n",
    " * Dwass, Steel, Critchlow, and Fligner test.\n",
    " * Mann-Whitney test.\n",
    " * Nashimoto and Wright (NPM) test.\n",
    " * Nemenyi test.\n",
    " * van Waerden test.\n",
    " * Wilcoxon test.\n",
    "\n",
    "which are for non-parametric designs. In Python we have all these functions in the `scipy.stats` package or from the `scipy.stats.mstats`. Mann-Whitney test, for example, require that the categories are also numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 6001,
     "status": "ok",
     "timestamp": 1572942013796,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "z05DBeKJZucq",
    "outputId": "e4813e14-cc5d-4628-a508-3c200ce7901f"
   },
   "outputs": [],
   "source": [
    "!pip install scikit_posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glb2MKBnZ2Xe"
   },
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1572942018718,
     "user": {
      "displayName": "Juan Jose Manjarin",
      "photoUrl": "",
      "userId": "04910883006985787828"
     },
     "user_tz": -60
    },
    "id": "RgAcVERUZ3qc",
    "outputId": "cbef90a3-eece-4dc8-b10d-9c303c0844ab"
   },
   "outputs": [],
   "source": [
    "sp.posthoc_conover(dataset_nona, val_col='lanwwith2014', group_col='continent', p_adjust = 'holm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_rVH8KfnfQa"
   },
   "source": [
    "In the table we see the p-values of the comparisons of the continents in groups of 2. If we were using a 1%, none of the differences would be significant. However, for a 10%, only the difference bewteen Europe and America would not be significant. In fact these results coincide with those of Tukey's post-hoc."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.9.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1+"
  },
  "source_map": [
   13,
   34,
   67,
   77,
   130,
   136,
   166,
   170,
   191,
   212,
   216,
   239,
   245,
   266,
   272,
   281,
   287,
   307,
   316,
   337,
   341,
   347,
   368,
   390,
   402,
   427,
   433,
   454,
   458,
   479,
   484,
   490,
   512,
   516,
   525,
   547,
   555,
   620,
   624,
   644,
   648,
   668,
   676,
   682,
   714,
   731,
   751,
   757,
   777
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}