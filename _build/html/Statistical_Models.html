
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>1. Statistical Models &#8212; Math and Stats</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1.5.1. Linear Regression" href="11_Linear_Model.html" />
    <link rel="prev" title="8. Power of the Test" href="10_Power_of_the_test.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Math and Stats</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Content/Intro.html">
   Preface
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Python
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Introduction_to_Python.html">
   1. Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Matplotlib.html">
   2. Matplotlib
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_Graphs_with_Pandas.html">
   3. Graphics with Pandas
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  EDA
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Central_Tendency.html">
   1. Central Tendency Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_Descriptive_Stats_with_Pandas.html">
   2. Descriptive Statistics with Pandas
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Probability Theory
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="06_Probability_Distribution_Functions.html">
   1. Probability Distribution Functions
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Inference Theory
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="07_Estimation_Theory.html">
   1. Estimation Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Confidence_Intervals.html">
   2.
   <font color="Red">
    Cases and Conditions
   </font>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_Confidence_Intervals.html#font-color-red-application-font">
   3.
   <font color="Red">
    Application
   </font>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Hypothesis_Testing.html">
   4. Hypothesis Testing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Hypothesis_Testing.html#font-color-red-types-of-tests-font">
   5.
   <font color="Red">
    Types of Tests
   </font>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Hypothesis_Testing.html#font-color-red-performing-a-test-font">
   6.
   <font color="Red">
    Performing a Test
   </font>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09_Hypothesis_Testing.html#font-color-red-python-tests-font">
   7.
   <font color="Red">
    Python Tests
   </font>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_Power_of_the_test.html">
   8. Power of the Test
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Statistical Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="current reference internal" href="#">
   1. Statistical Models
  </a>
  <ul class="collapse-ul">
   <li class="toctree-l2">
    <a class="reference internal" href="11_Linear_Model.html">
     1.5.1. Linear Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="12_ANOVA.html">
     1.5.2. ANOVA
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="13_Time_Series.html">
     1.5.3. Time Series
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05_Association_Rules.html">
   2.
   <font color="0061c7">
    Association Rules
   </font>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="061_Naive_Bayes.html">
   3.
   <font color="0061c7">
    Naïve Bayes
   </font>
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Statistical_Models.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://jjmanjarin.github.io/MathNStats"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://jjmanjarin.github.io/MathNStats/issues/new?title=Issue%20on%20page%20%2FStatistical_Models.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction-vs-estimation">
   1.1. Prediction vs. Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">
   1.2. Supervised vs. Unsupervised Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regression-vs-classification-models">
   1.3. Regression vs. Classification Models
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#validation-of-the-model">
   1.4. Validation of the Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bias-variance-trade-off">
   1.5. Bias-Variance Trade-off
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="statistical-models">
<h1><span class="section-number">1. </span>Statistical Models<a class="headerlink" href="#statistical-models" title="Permalink to this headline">¶</a></h1>
<p>When we have a complex data set the process of statistical learning allow us to modelize and understand it. Under this umbrella lie many different approaches and we will only deal here with the most basic ones. But before entering into the details, we may need a brief discussion of what is understood about modeling and, most importantly what we can and cannot obtain from the models we build.</p>
<p>We have already mentioned the types of variables that we can find in a data set with respect to their role, among all the possible names we have {\bf{response}}, {\bf{predictors}} and {\bf{confounding}}. Remember that the response are the dependent or endoenous variables and the predictors are also known as independent, exogenous or features, to mention just a few of their names. The way we will always denote them is <span class="math notranslate nohighlight">\(Y\)</span> as the response variable and <span class="math notranslate nohighlight">\(X\)</span> as the predictors and we must keep in mind the generally <span class="math notranslate nohighlight">\(X\)</span> will be a vector of variables, i.e. it will generally make reference to a whole set of independent variables.</p>
<p>In this general context, modeling the behavior of the response in terms of the predictors mean that we say</p>
<div class="amsmath math notranslate nohighlight" id="equation-bc0f2c4b-7ab9-42f4-b159-3febd66d2180">
<span class="eqno">(1.96)<a class="headerlink" href="#equation-bc0f2c4b-7ab9-42f4-b159-3febd66d2180" title="Permalink to this equation">¶</a></span>\[\begin{equation}
Y = f(X)+u
\end{equation}\]</div>
<p>In this general form <span class="math notranslate nohighlight">\(f(X)\)</span> corresponds to the systematic part of the response and <span class="math notranslate nohighlight">\(u\)</span> to the random errors, and the process of estimation or modelling is that of finding a best fit functional form for <span class="math notranslate nohighlight">\(f(X)\)</span> which will always be dependent of the data set we have. The main reasons behind estimation are {\bf{prediction}} and {\bf{inference}}.</p>
<div class="section" id="prediction-vs-estimation">
<h2><span class="section-number">1.1. </span>Prediction vs. Estimation<a class="headerlink" href="#prediction-vs-estimation" title="Permalink to this headline">¶</a></h2>
<p>We talk about {\bf{prediction}} when we want to use the estimated model</p>
<div class="amsmath math notranslate nohighlight" id="equation-9c789e80-2663-4ff6-8a96-cf028e5743e5">
<span class="eqno">(1.97)<a class="headerlink" href="#equation-9c789e80-2663-4ff6-8a96-cf028e5743e5" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\hat Y = \hat f(X)
\end{equation}\]</div>
<p>to obtain particular values of the response variable. A rather common example may be that of predicting the income of a person in terms of the years of education.</p>
<p>In this sense we must understand that, in general, the estimated value will be different from the measured value for the same <span class="math notranslate nohighlight">\(X\)</span> and even more, both of them will be different from the actual true value which we will never know. This allow us to compute the MSE of the model as</p>
<div class="amsmath math notranslate nohighlight" id="equation-1e2f5849-6d1f-4130-8555-3b32c41ba55d">
<span class="eqno">(1.98)<a class="headerlink" href="#equation-1e2f5849-6d1f-4130-8555-3b32c41ba55d" title="Permalink to this equation">¶</a></span>\[\begin{equation}
E[(Y-\hat Y)^2]=[bias(\hat f(X)]^2+Var(\hat f(X))+Var(u)
\end{equation}\]</div>
<p>which is splitted in an irreducible term defined by the variance of the error term and another reducible written as the MSE of the estimator defined by the model. In the context of Statistical Decision Theory, this expected value is known as the {\bf{expected predition error}} (EPE) and is the expected value of the {\bf{loss function}}.</p>
<p>It is the minimization of this EPE function is what leads to the different models we will see in this chapter and in general all the models try to minimize the reducible part of the function.</p>
<p>If we are not interested in predicting values of the response but on seen the way in which <span class="math notranslate nohighlight">\(Y\)</span> is affected by the different <span class="math notranslate nohighlight">\(X\)</span> we talk about {\bf{inference}} of the model. It is in this context where all the notions of hypothesis testing come to our rescue: which predictors are associated to the response? Which set of variables can we use to describe the behavior of the response? What is the individual relation between predictors and response?</p>
<p>The estimation processes we will see along this chapter can be described as parametric, in the sense that we will make an explicit assumption on the model for <span class="math notranslate nohighlight">\(f(X)\)</span> and then try to estimate it. This is what we do with the linear regression or probability models, where we assume that the functional form of the response as a function of <span class="math notranslate nohighlight">\(X\)</span> is that of a linear function</p>
<div class="amsmath math notranslate nohighlight" id="equation-3f0ead20-c1e9-449a-83cb-37ac6d41ddc6">
<span class="eqno">(1.99)<a class="headerlink" href="#equation-3f0ead20-c1e9-449a-83cb-37ac6d41ddc6" title="Permalink to this equation">¶</a></span>\[\begin{equation}
Y=\beta_0+\beta_i\,X_i
\end{equation}\]</div>
<p>The main parametric procedure is that of {\bf{least squares}} that we will shortly view.</p>
<p>Just as a comment, we must keep in mind that this is not the only approach, for example, the {\bf{splines}} are non-parametric since they do not assume any specific functional form. Each procedure has its own pros and cons that are somewhat far from the scope of the course.</p>
<p>There is, however, one potential problem with parametric models and it is that since we do not really know the population model, the estimated model will not follow the actual true values. The idea is that in general they are designed to capture a particular property of the population through a simplication of the true overall behavior.</p>
</div>
<div class="section" id="supervised-vs-unsupervised-learning">
<h2><span class="section-number">1.2. </span>Supervised vs. Unsupervised Learning<a class="headerlink" href="#supervised-vs-unsupervised-learning" title="Permalink to this headline">¶</a></h2>
<p>In our previous discussion we have implicitely assumed that we know the response variables. However, that is not always the case, a for example in a market segmentation study, where we split the population according to some of their properties: age, income, zip code or any other, but we want to understand the spending habits of that population. These last partterns will never be known for us.</p>
<p>If we know the response variable we talk about {\bf{supervised learning}}, in the sense that the response let us see how close to it we are with our model and then, in some sense it is supervising us. However, if we do not know it, we speak of an {\bf{unsupervised learning}}. In this context, the regression and classification linear models we will study are always supervised procedures.</p>
</div>
<div class="section" id="regression-vs-classification-models">
<h2><span class="section-number">1.3. </span>Regression vs. Classification Models<a class="headerlink" href="#regression-vs-classification-models" title="Permalink to this headline">¶</a></h2>
<p>Another important differentiation in the models is that of regression and classification. In this case we are making reference to the nature of the response variable. If the response variable is quantitative, the model will be a {\bf{regression}}, but if the response is qualitative, the model will be a {\bf{classification}}, then if we want to study how gender affects income, that will be a regression problem. However, if we want to see if we may determine the gender from the income, we will be classifying the population.</p>
</div>
<div class="section" id="validation-of-the-model">
<h2><span class="section-number">1.4. </span>Validation of the Model<a class="headerlink" href="#validation-of-the-model" title="Permalink to this headline">¶</a></h2>
<p>The general timeline in the process of building any model is the following</p>
<div class="amsmath math notranslate nohighlight" id="equation-47829763-4ef4-4842-8fcc-9228c4810c1f">
<span class="eqno">(1.100)<a class="headerlink" href="#equation-47829763-4ef4-4842-8fcc-9228c4810c1f" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\text{Training} \longrightarrow \text{Validation} \longrightarrow \text{Test}
\end{equation}\]</div>
<p>In this context we must keep in mind that the training and testing must be done on different data sets which receive, accordingly the names of {\bf{training set}} and {\bf{test set}}. In R we can perform a splitting of our sample using the following code\</p>
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">111</span><span class="p">)</span>
<span class="n">sampling</span> <span class="o">&lt;-</span> <span class="nf">sample.int</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nf">nrow</span><span class="p">(</span><span class="n">mydata.comp</span><span class="p">),</span>
                       <span class="n">size</span><span class="o">=</span><span class="nf">floor</span><span class="p">(</span><span class="m">0.8</span><span class="o">*</span><span class="nf">nrow</span><span class="p">(</span><span class="n">mydata.comp</span><span class="p">)),</span>
                       <span class="n">replace</span> <span class="o">=</span> <span class="kc">FALSE</span><span class="p">)</span>

<span class="n">training.set</span> <span class="o">&lt;-</span> <span class="n">mydata</span><span class="p">[</span><span class="n">sampling</span><span class="p">,]</span>
<span class="n">test.set</span> <span class="o">&lt;-</span> <span class="n">mydata</span><span class="p">[</span><span class="o">-</span><span class="n">sampling</span><span class="p">,]</span>
</pre></div>
</div>
<p>where we have followed a 80-20 rule, so that the <span class="math notranslate nohighlight">\(80\%\)</span> of the sample is used for the training of the model and the remaining <span class="math notranslate nohighlight">\(20\%\)</span> will be considered as the test set.</p>
<p>The validation procedure is understood as the {\bf{model selection}}, i.e. the procedure in which we estimate the performance of different models by including or excluding variables and through the study of all the potential problems that may arise in the training (outliers, heteroskedasticity,…. On the hand, the testing is understood as the {\bf{model assessment}} where, once found the final model we assess the model by estimating the error on the test data set.</p>
<p>Once we make a difference in the training and test sets, we can see that the idea is that the model built from the training data should be accurate enough once we use it in previously unseen data. If we have, for example, built a model from clinical measurements in a study for the prediction of diabetes or to infer their relation with diabetes, we want to be able to use it with future patients. Mathematically we want a model that gives the lowest MSE for the test set, not for the training set.</p>
<p>In some cases we want to build flexible statistical models so that we decrease the value of the training MSE, but it may occur that the MSE increases with the flexibility. In these cases the model suffers what is known as {\bf{overfitting}} which implies that our model is finding some patterns due to random effects and not to the true behavior, i.e. it is following the white noise of the model.</p>
</div>
<div class="section" id="bias-variance-trade-off">
<h2><span class="section-number">1.5. </span>Bias-Variance Trade-off<a class="headerlink" href="#bias-variance-trade-off" title="Permalink to this headline">¶</a></h2>
<p>From the equation for the EPE, we see that the way to minimize it is by choosing a method that produces both, a low bias and a low variance. The variance we are interested on is the amount by which the estimated model would change if we use a different training set, while the bias refers to the error introduced by choosing an approximation to the real life problem.</p>
<p>In flexible models, the variance will increase and the bias will decrease, i.e. we will be closer to the values but in a very unstable way (this is the case of {\bf{K-nearest neighbourhood method}}). On the other hand with restricted models, we obtain models with low variance but large bias, i.e. maybe not close to the points but very stable (this is the case of the {\bf{linear regression}}). It would be an error thinking that a flexible model may overperform another simpler and more restricted, in fact it is not unsual to find the contrary situation, a counterintuitive situation that arises due to the overfitting problem.</p>
<p>It is relatively easy to deal with methods that gives a low value of the bias or of the variance, but not together. In real life it is almost always imposible to find the value of the MSE, bias or variance for the test set but we must always keep in mind that the methods we use deal with a {\bf{bias-variance trade-off}} in which we typically minimize one property or another.</p>
<div class="toctree-wrapper compound">
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="10_Power_of_the_test.html" title="previous page"><span class="section-number">8. </span>Power of the Test</a>
    <a class='right-next' id="next-link" href="11_Linear_Model.html" title="next page"><span class="section-number">1.5.1. </span>Linear Regression</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Juanjo Manjarín<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>